# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1chBnSWnDMOSNAqcoPMFopOeF-9fHan-_
"""

## Importing the requirements

import torch
import torch.nn as nn

## Setting the device
device = 'cuda' if torch.cuda.is_available() else 'cpu'

## Refer to the page 3, of the paper Chargrid-OCR: End-to-end Trainable Optical Character Recognition for Printed Documents using Instance Segmentation
## Batch Normalization [15] and ReLu activations [17] are applied after each intermediate convolution.

## This block would be responsible for block 1 for the encoding part 
def encoder_block_1(in_channels, out_channels):

  ## args would be C, C/2, where C is the base channel for the document
  conv_block = nn.Sequential(nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = (3, 3), padding = 1), 
                             nn.BatchNorm2d(num_features = out_channels), 
                             nn.ReLU())
  return conv_block

## https://github.com/pytorch/pytorch/issues/3867#issuecomment-974159134

import collections
from itertools import repeat
import torch
from torch import nn
import torch.nn.functional as F

def _ntuple(n):
    """Copied from PyTorch since it's not importable as an internal function

    https://github.com/pytorch/pytorch/blob/v1.10.0/torch/nn/modules/utils.py#L6
    """
    def parse(x):
        if isinstance(x, collections.abc.Iterable):
            return tuple(x)
        return tuple(repeat(x, n))

    return parse


_pair = _ntuple(2)

class Conv2dSame(nn.Module):
    """Manual convolution with same padding

    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword argument,
    this does not export to CoreML as of coremltools 5.1.0, so we need to
    implement the internal torch logic manually. Currently the ``RuntimeError`` is

    "PyTorch convert function for op '_convolution_mode' not implemented"

    Also same padding is not supported for strided convolutions at the moment
    https://github.com/pytorch/pytorch/blob/v1.10.0/torch/nn/modules/conv.py#L93
    """

    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs):
        """Wrap base convolution layer

        See official PyTorch documentation for parameter details
        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html
        """
        super().__init__()
        self.conv = nn.Conv2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            dilation=dilation,
            **kwargs)

        # Setup internal representations
        kernel_size_ = _pair(kernel_size)
        dilation_ = _pair(dilation)
        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)

        # Follow the logic from ``nn._ConvNd``
        # https://github.com/pytorch/pytorch/blob/v1.10.0/torch/nn/modules/conv.py#L116
        for d, k, i in zip(dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)):
            total_padding = d * (k - 1)
            left_pad = total_padding // 2
            self._reversed_padding_repeated_twice[2 * i] = left_pad
            self._reversed_padding_repeated_twice[2 * i + 1] = (
                    total_padding - left_pad)

    def forward(self, imgs):
        """Setup padding so same spatial dimensions are returned

        All shapes (input/output) are ``(N, C, W, H)`` convention

        :param torch.Tensor imgs:
        :return torch.Tensor:
        """
        padded = F.pad(imgs, self._reversed_padding_repeated_twice)
        return self.conv(padded)

def encoder_block_2(in_channels,out_channels,dilation_orange_block = 1, dilation_blue_block = 1):

  
  first_conv_block = nn.Sequential(Conv2dSame(in_channels = in_channels, out_channels = out_channels, kernel_size = (3, 3), stride = 2,
                                             dilation = dilation_orange_block),
                                   nn.BatchNorm2d(num_features = out_channels), 
                                   nn.ReLU())
  
  second_conv_block = nn.Sequential(Conv2dSame(in_channels = out_channels, out_channels = out_channels, kernel_size = (3, 3), stride = 1,
                                              dilation = dilation_blue_block),
                                     nn.BatchNorm2d(num_features = out_channels), 
                                   nn.ReLU())
  
  third_conv_block = nn.Sequential(Conv2dSame(in_channels = out_channels, out_channels = out_channels, kernel_size = (3, 3), stride = 1,
                                            dilation = dilation_blue_block),
                                    nn.BatchNorm2d(num_features = out_channels), 
                                   nn.ReLU())

  conv_block = nn.Sequential(first_conv_block, 
                             second_conv_block,
                             third_conv_block)
  return conv_block

def encoder_block_3(in_channels,out_channels,dilation_blue_block=1):

  
  first_conv_block = nn.Sequential(Conv2dSame(in_channels = in_channels, out_channels = out_channels, kernel_size = (3, 3), stride = 1,
                                             dilation=dilation_blue_block),
                                   nn.BatchNorm2d(num_features = out_channels), 
                                   nn.ReLU())
  
  second_conv_block = nn.Sequential(Conv2dSame(in_channels = out_channels, out_channels = out_channels, kernel_size = (3, 3), stride = 1,
                                              dilation = dilation_blue_block),
                                     nn.BatchNorm2d(num_features = out_channels), 
                                   nn.ReLU())
  
  third_conv_block = nn.Sequential(Conv2dSame(in_channels = out_channels, out_channels = out_channels, kernel_size = (3, 3), stride = 1,
                                            dilation = dilation_blue_block),
                                    nn.BatchNorm2d(num_features = out_channels), 
                                   nn.ReLU())

  conv_block = nn.Sequential(first_conv_block, 
                             second_conv_block,
                             third_conv_block)
  return conv_block

class Encoder(nn.Module):

  def __init__(self, in_channels):
    super(Encoder, self).__init__()

    self.block_1 = encoder_block_1(in_channels = in_channels, out_channels = in_channels//2)
    
    self.block_2 = encoder_block_2(in_channels = in_channels//2, out_channels = in_channels,
                                   dilation_orange_block = 1, dilation_blue_block = 1)
    
    self.block_3 = encoder_block_2(in_channels = in_channels, out_channels = 2*in_channels, 
                                   dilation_orange_block = 1, dilation_blue_block = 1)
    
    self.block_4 = encoder_block_2(in_channels = 2*in_channels, out_channels = 4*in_channels, 
                                   dilation_orange_block = 2, dilation_blue_block = 2)
    
    self.block_5 = encoder_block_3(in_channels = 4*in_channels, out_channels = 8*in_channels,
                                   dilation_blue_block = 4)
    
    self.block_6 = encoder_block_3(in_channels = 8*in_channels, out_channels = 8*in_channels,
                                   dilation_blue_block = 8)

  def forward(self, x):
    first_output = self.block_1(x)

    ## second, third, forth output is in skip connection
    second_output = self.block_2(first_output)
    third_output = self.block_3(second_output)
    fourth_output = self.block_4(third_output)

    ## sixth output is useful for decoding
    fifth_output = self.block_5(fourth_output)
    sixth_output = self.block_6(fifth_output)

    ## Dictionary elements would be responsible for skip connections
    return {
        'second_output': second_output,
            'third_output': third_output,
            'fourth_output': fourth_output, 
            'sixth_output': sixth_output}

## For the sake of testing

in_channels = 32
target_size = (1648,1272)
batch_size = 2

sample_batch_images = torch.rand(batch_size, in_channels, *target_size)
encoder_block = Encoder(in_channels = in_channels)

if __name__ == '__main__':
  sample_batch_images = sample_batch_images
  encoder_block = encoder_block
  encodings = encoder_block(sample_batch_images)

  for key in list(encodings.keys()):
    print_statement = '{0: <50}'.format(str(key) + " has a shape:")
    print(print_statement, encodings[key].shape)

class semanticSegmentationDecoder(nn.Module):

  def __init__(self,in_channels,out_channels):
    super(semanticSegmentationDecoder,self).__init__()

    self.block_1 = nn.Sequential(nn.Conv2d(in_channels = 12 * in_channels,out_channels=4*in_channels,kernel_size=1,stride = 1), 
                                 nn.BatchNorm2d(num_features = 4 * in_channels), 
                                 nn.ReLU(),
                                 nn.ConvTranspose2d(in_channels=4*in_channels,out_channels=4*in_channels,kernel_size=3,stride=2,
                                                    padding=1,output_padding = 1),
                                 nn.BatchNorm2d(num_features = 4 * in_channels),  
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = 4*in_channels, out_channels=4*in_channels, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(num_features = 4 * in_channels),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = 4*in_channels, out_channels=4*in_channels, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(num_features = 4 * in_channels),
                                 nn.ReLU(),
                                )
    
    self.block_2 = nn.Sequential(nn.Conv2d(in_channels = 6 * in_channels,out_channels=2*in_channels,kernel_size=1,stride = 1), 
                                 nn.BatchNorm2d(num_features = 2 * in_channels), 
                                 nn.ReLU(),
                                 nn.ConvTranspose2d(in_channels=2*in_channels,out_channels=2*in_channels,kernel_size=3,stride=2,
                                                    padding=1,output_padding = 1),
                                 nn.BatchNorm2d(num_features = 2 * in_channels),  
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = 2*in_channels, out_channels=2*in_channels, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(num_features = 2 * in_channels),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = 2*in_channels, out_channels=2*in_channels, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(num_features = 2 * in_channels),
                                 nn.ReLU(),
                                )
    
    self.block_3 = nn.Sequential(nn.Conv2d(in_channels = 3 * in_channels,out_channels= in_channels,kernel_size=1,stride = 1), 
                                 nn.BatchNorm2d(num_features = in_channels), 
                                 nn.ReLU(),
                                 nn.ConvTranspose2d(in_channels=in_channels,out_channels=in_channels,kernel_size=3,stride=2,
                                                    padding=1,output_padding = 1),
                                 nn.BatchNorm2d(num_features = in_channels),  
                                 nn.ReLU()
                                 )
    

    self.block_4 = nn.Sequential(nn.Conv2d(in_channels = in_channels,out_channels = in_channels,kernel_size=3,stride = 1), 
                                 nn.BatchNorm2d(num_features = in_channels), 
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = in_channels,out_channels = in_channels,kernel_size=3,stride=1),
                                 nn.BatchNorm2d(num_features = in_channels),  
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size=3,stride=1,padding=1)
                                 )
    
  
  def forward(self,x):

     # x: A list of 4 outputs from encoder

     encoder_input = torch.cat([x['fourth_output'], x['sixth_output']], dim=1)
     first_output = self.block_1( encoder_input)

     first_input = torch.cat([x['third_output'], first_output], dim=1)
     second_output = self.block_2(first_input)

     second_input = torch.cat([x['second_output'], second_output], dim=1)
     third_output = self.block_3(second_input)

     fourth_output = self.block_4(third_output)
     return fourth_output

out_channels = 64

decodings = semanticSegmentationDecoder(in_channels=32,out_channels=64)

decoded = decodings(encodings)

print(decoded)

class BoundingBoxRegressionDecoder(nn.Module):

  def __init__(self,in_channels,number_of_anchors):
    super(BoundingBoxRegressionDecoder,self).__init__()

    self.block_1 = nn.Sequential(nn.Conv2d(in_channels = 12 * in_channels,out_channels=4*in_channels,kernel_size=1,stride = 1), 
                                 nn.BatchNorm2d(num_features = 4 * in_channels), 
                                 nn.ReLU(),
                                 nn.ConvTranspose2d(in_channels=4*in_channels,out_channels=4*in_channels,kernel_size=3,stride=2,
                                                    padding=1,output_padding = 1),
                                 nn.BatchNorm2d(num_features = 4 * in_channels),  
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = 4*in_channels, out_channels=4*in_channels, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(num_features = 4 * in_channels),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = 4*in_channels, out_channels=4*in_channels, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(num_features = 4 * in_channels),
                                 nn.ReLU(),
                                )
    
    self.block_2 = nn.Sequential(nn.Conv2d(in_channels = 6 * in_channels,out_channels=2*in_channels,kernel_size=1,stride = 1), 
                                 nn.BatchNorm2d(num_features = 2 * in_channels), 
                                 nn.ReLU(),
                                 nn.ConvTranspose2d(in_channels=2*in_channels,out_channels=2*in_channels,kernel_size=3,stride=2,
                                                    padding=1,output_padding = 1),
                                 nn.BatchNorm2d(num_features = 2 * in_channels),  
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = 2*in_channels, out_channels=2*in_channels, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(num_features = 2 * in_channels),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = 2*in_channels, out_channels=2*in_channels, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(num_features = 2 * in_channels),
                                 nn.ReLU(),
                                )
    
    self.block_3 = nn.Sequential(nn.Conv2d(in_channels = 3 * in_channels,out_channels= in_channels,kernel_size=1,stride = 1), 
                                 nn.BatchNorm2d(num_features = in_channels), 
                                 nn.ReLU(),
                                 nn.ConvTranspose2d(in_channels=in_channels,out_channels=in_channels,kernel_size=3,stride=2,
                                                    padding=1,output_padding = 1),
                                 nn.BatchNorm2d(num_features = in_channels),  
                                 nn.ReLU()
                                 )
    

    self.block_4 = nn.Sequential(nn.Conv2d(in_channels = in_channels,out_channels = in_channels,kernel_size=3,stride = 1), 
                                 nn.BatchNorm2d(num_features = in_channels), 
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = in_channels,out_channels = in_channels,kernel_size=3,stride=1),
                                 nn.BatchNorm2d(num_features = in_channels),  
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = in_channels,out_channels = 2*number_of_anchors,kernel_size=3,stride=1,padding=1)
                                 )
    
    self.block_5 = nn.Sequential(nn.Conv2d(in_channels = in_channels,out_channels = in_channels,kernel_size=3,stride = 1), 
                                 nn.BatchNorm2d(num_features = in_channels), 
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = in_channels,out_channels = in_channels,kernel_size=3,stride=1),
                                 nn.BatchNorm2d(num_features = in_channels),  
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels = in_channels,out_channels = 4*number_of_anchors,kernel_size=3,stride=1,padding=1)
                                 )
    
  
  def forward(self,x):

     # x: A list of 4 outputs from encoder

     encoder_input = torch.cat([x['fourth_output'], x['sixth_output']], dim=1)
     first_output = self.block_1( encoder_input)

     first_input = torch.cat([x['third_output'], first_output], dim=1)
     second_output = self.block_2(first_input)

     second_input = torch.cat([x['second_output'], second_output], dim=1)
     third_output = self.block_3(second_input)

     fourth_output_1 = self.block_4(third_output)
     fourth_output_2 = self.block_5(third_output)

     return fourth_output_1,fourth_output_2

bounding =  BoundingBoxRegressionDecoder(in_channels=32,number_of_anchors=1)

a,b = bounding(encodings)

print("fourth_output",a)

print("fourth_output",b)

class Chargrid2D(nn.Module):
    def __init__(self, input_channels, n_classes, n_a=1, base_channels=16):
        super(Chargrid2D, self).__init__()

        self.encoder = Encoder(input_channels)
        self.semantic_segmentation_decoder = semanticSegmentationDecoder(input_channels,n_classes)
        self.bounding_box_regression_decoder = BoundingBoxRegressionDecoder(input_channels,n_a)

    def forward(self, x):
        x = self.encoder(x)
        y1 = self.semantic_segmentation_decoder(x)
        y2, y3 = self.bounding_box_regression_decoder(x)

        return y1, y2, y3

model = Chargrid2D(input_channels=302, n_classes=10)
x = torch.ones((1, 302, 512, 512))
y1, y2, y3 = model(x)
print("y1 size")
print(y1.size())
print("y2 size")
print(y2.size())
print("y3 size")
print(y3.size())